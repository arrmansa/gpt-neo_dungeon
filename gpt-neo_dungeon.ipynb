{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdtMW21T9pDl"
   },
   "source": [
    "# Instructions\n",
    "**This notebook contains two custom finetuned models called 2.7B-horni and 2.7B-horni-ln which are NOT official EleutherAI models or affiliated with them in any way beyond using their model as a basis. You can also select the original EleutherAI/gpt-neo-2.7B model in the model selection's dropdown menu.**\n",
    "\n",
    "Go through each cell in this notebook one by one, take a look at the options and descriptions and then press the play button to the left of it. You can skip the optional one. Don't skip any of the others. After running the \"Play\" cell, a small form will appear underneath, which you can use to actually play. Check out this [guide](https://gitlab.com/nolialsea/novelaicontributions/-/blob/master/Guides/Using%20finetuneanon%27s%20Colab.md) for details.\n",
    "\n",
    "To reset the state of your game, run the \"Setup\" cell again. Closing the notebook will lose your progress, so if you want to keep your story, use the \"history\" action, copy out your story to a text editor. You can also copy out your author's note and memory from the output of the \"info\" action.\n",
    "\n",
    "The most reliable way of loading the models is to store them in your google drive. This notebook will automatically download the models from a google drive in the model setup step. If this succeeds, you can then copy it into your drive in the optional following step. You can also download the files yourself and upload them to your drive yourself.\n",
    "\n",
    "* [2.7B-horni](https://mega.nz/file/6BNykLJb#B6gxK3TnCKBpeOF1DJMXwaLc_gcTcqMS0Lhzr1SeJmc) [[Google Drive](https://drive.google.com/file/d/1-Jj_hlyNCQxuSnK7FFBXREGnRSMI5MoF/view?usp=sharing)] 5GB, for NSFW styled output\n",
    "* [2.7B-horni-ln](https://mega.nz/file/rQcWCTZR#tCx3Ztf_PMe6OtfgI95KweFT5fFTcMm7Nx9Jly_0wpg) [[Google Drive](https://drive.google.com/file/d/1M1JY459RBIgLghtWDRDXlD4Z5DAjjMwg/view?usp=sharing)] 5GB, for light novel styled output\n",
    "* Torrent: magnet:?xt=urn:btih:31d956ff4a248dcf914b1b7e474cbac02d70d6a4&dn=gtp-neo-horni&tr=http%3A%2F%2Fopenbittorrent.com%3A80%2Fannounce\n",
    "\n",
    "Anon says about Google Drive: If you run into quotas, create a folder in your own google drive, add a shortcut to the file in question in said folder, and then download the folder itself instead of the file directly.\n",
    "\n",
    "If you have an GTX card with 8GB or more, you may be able to run this notebook locally. If you integrate the models in clover edition, use the transformers fork that's installed in the setup cell of this notebook to avoid OOM errors and use model.generate() to be able to use repetition_penalty_range and repetition_penalty_slope. For running locally, remove the map_device arguments from torch.load calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ucP5hOdoMgzC"
   },
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "#@markdown Run this for setting up dependencies or resetting actions\n",
    "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-dungeon-localattention2\n",
    "#!wget -c http://ftp.us.debian.org/debian/pool/main/m/megatools/megatools_1.11.0~git20200404-1_amd64.deb -O megatools.deb\n",
    "#!dpkg -i megatools.deb\n",
    "!pip install gdown\n",
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "import tarfile\n",
    "import codecs\n",
    "import torch\n",
    "import subprocess\n",
    "import copy\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "\n",
    "try:\n",
    "  initialized += 1\n",
    "except:\n",
    "  get_ipython().events.register('pre_run_cell', set_css)\n",
    "  tail_free_sampling, top_k, top_p, temperature, number_generated_tokens, repetition_penalty, repetition_penalty_range, repetition_penalty_slope, number_show_last_actions = 0.95, 60, 0.9, 0.8, 40, 1.25, 300, 3.33, 15\n",
    "  prevent_square_brackets, prevent_angle_brackets, prevent_curly_brackets = True, True, True\n",
    "  enable_top_k, enable_top_p, enable_tfs = False, False, True\n",
    "  bad_words_ids = None\n",
    "  initialized = 0\n",
    "\n",
    "actions = []\n",
    "memory = (\"\", torch.zeros((1, 0)).long())\n",
    "lmi = [\"\", torch.zeros((1, 0)).long()]\n",
    "an = (\"\", torch.zeros((1, 0)).long())\n",
    "an_depth = 3\n",
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "M3ZF4eCFMV6E"
   },
   "outputs": [],
   "source": [
    "#@title Model setup\n",
    "#@markdown horni was finetuned for one epoch on about 800MB worth of random blocks of text from literotica. Do not use the horni model if you dislike NSFW outputs. horni-ln uses horni as a base and was finetuned for one epoch on 579MB of text from a light novel dataset.\n",
    "\n",
    "print(\"Setting up model, this will take a few minutes\")\n",
    "\n",
    "model_name = \"2.7B-horni\" #@param [\"2.7B-horni\", \"2.7B-horni-ln\", \"EleutherAI/gpt-neo-2.7B\"]\n",
    "model_gdrive = \"/content/drive/MyDrive/gpt-neo-2.7B-horni.tar\" #@param {type:\"string\"}\n",
    "use_gdrive = False #@param {type:\"boolean\"}\n",
    "#@markdown If you download errors, the google drive downloads might be over their daily download quota. In that case, right-click, select \"interrupt execution\", download the checkpoint from mega yourself, upload to your google drive, tick use_gdrive and put the correct filename, e.g. `gpt-neo-2.7B-horni-ln.tar` and restart the cell.\n",
    "#@markdown\n",
    "#@markdown Warnings about certain attention bias parameters being uninitialized or about the google drive already having been mounted can be ignored.\n",
    "\n",
    "custom_models = [\"2.7B-horni\", \"2.7B-horni-ln\"]\n",
    "\n",
    "if use_gdrive:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "#model_types = {\"2.7B-horni\": \"https://mega.nz/file/6BNykLJb#B6gxK3TnCKBpeOF1DJMXwaLc_gcTcqMS0Lhzr1SeJmc\",\n",
    "#               \"2.7B-horni-ln\": \"https://mega.nz/file/rQcWCTZR#tCx3Ztf_PMe6OtfgI95KweFT5fFTcMm7Nx9Jly_0wpg\"}\n",
    "model_types = {\"2.7B-horni\": \"https://drive.google.com/uc?id=1-Jj_hlyNCQxuSnK7FFBXREGnRSMI5MoF\",\n",
    "               \"2.7B-horni-ln\": \"https://drive.google.com/uc?id=1M1JY459RBIgLghtWDRDXlD4Z5DAjjMwg\"}\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "pipeline = None\n",
    "checkpoint = None\n",
    "\n",
    "if not os.path.isdir(\"gpt-neo-\"+model_name) and model_name in custom_models:\n",
    "  if use_gdrive:\n",
    "    tar = tarfile.open(model_gdrive, \"r\")\n",
    "  else:\n",
    "    model_url = model_types[model_name]\n",
    "    print(\"Downloading:\", model_url)\n",
    "    #!megadl $model_url --no-ask-password\n",
    "    !gdown $model_url\n",
    "    tar = tarfile.open(model_name + \".tar\", \"r\")\n",
    "  tar.extractall()\n",
    "  tar.close()\n",
    "\n",
    "if model_name in custom_models:\n",
    "  checkpoint = torch.load(\"gpt-neo-\" + model_name + \"/pytorch_model.bin\", map_location=\"cpu\")\n",
    "  model = GPTNeoForCausalLM.from_pretrained(\"gpt-neo-\" + model_name, state_dict=checkpoint).half().to(\"cpu\").eval()\n",
    "  for k in list(checkpoint.keys()):\n",
    "    del checkpoint[k]\n",
    "  del checkpoint\n",
    "else:\n",
    "  from transformers.file_utils import cached_path, WEIGHTS_NAME, hf_bucket_url\n",
    "  archive_file = hf_bucket_url(model_name, filename=WEIGHTS_NAME)\n",
    "  resolved_archive_file = cached_path(archive_file)\n",
    "  checkpoint = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
    "  for k in checkpoint.keys():\n",
    "    checkpoint[k] = checkpoint[k].half()\n",
    "  model = GPTNeoForCausalLM.from_pretrained(model_name, state_dict=checkpoint).half().to(\"cpu\").eval()\n",
    "  for k in list(checkpoint.keys()):\n",
    "    del checkpoint[k]\n",
    "  del checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\"\"\"\n",
    "if torch.cuda.get_device_properties(0).total_memory > 15000 * 1024 * 1024:\n",
    "  print(\"Big GPU detected, using fp32\")\n",
    "  model = model.float()\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wZFSWPEP1U2b"
   },
   "outputs": [],
   "source": [
    "#@title Copy downloaded model to google drive (optional)\n",
    "#@markdown If the model checkpoint was downloaded automatically in the previous step, you can copy it to your google drive here for more reliable access in the future\n",
    "gdrive_target = \"/content/drive/MyDrive/gpt-neo-2.7B-horni.tar\" #@param {type:\"string\"}\n",
    "copy_model_file = False #@param {type:\"boolean\"}\n",
    "\n",
    "if copy_model_file:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  model_tar = '/content/' + model_name + \".tar\"\n",
    "  !cp -v $model_tar $gdrive_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Ls_x_sSLN2hU"
   },
   "outputs": [],
   "source": [
    "#@title Sampling settings (DO NOT SKIP)\n",
    "#@markdown You can modify sampling settings here. Don't forget to run the cell again after changing. The number of generated tokens is subtracted from the context window size, don't set it high.\n",
    "tail_free_sampling = 0.95 #@param {type:\"number\"}\n",
    "top_k = 60 #@param {type:\"number\"}\n",
    "top_p = 0.9 #@param {type:\"number\"}\n",
    "temperature =  0.8#@param {type:\"number\"}\n",
    "number_generated_tokens =  40#@param {type:\"integer\"}\n",
    "repetition_penalty = 1.25 #@param {type:\"number\"}\n",
    "repetition_penalty_range = 512 #@param {type:\"number\"}\n",
    "repetition_penalty_slope = 3.33 #@param {type:\"number\"}\n",
    "number_show_last_actions = 15 #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown If tail free sampling is enabled, top_p and top_k should probably not be used.\n",
    "enable_tfs = True #@param {type:\"boolean\"}\n",
    "enable_top_k = False #@param {type:\"boolean\"}\n",
    "enable_top_p = False #@param {type:\"boolean\"}\n",
    "\n",
    "if not enable_tfs:\n",
    "  tail_free_sampling = None\n",
    "if not enable_top_k:\n",
    "  top_k = None\n",
    "if not enable_top_p:\n",
    "  top_p = None\n",
    "\n",
    "#@markdown Temperatures seem to give results different from those in AID, so play around with it. Even 0.5 can give good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IYeGnR7uzR4d"
   },
   "outputs": [],
   "source": [
    "#@title Prevent tokens like [], <> and {} from being generated\n",
    "#thanks STARSTRUCK\n",
    "\n",
    "prevent_square_brackets = True #@param {type:\"boolean\"}\n",
    "prevent_angle_brackets = True #@param {type:\"boolean\"}\n",
    "prevent_curly_brackets = True #@param {type:\"boolean\"}\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_keys = vocab.keys()\n",
    "bad_keys = list()\n",
    "find_keys = lambda char : [key for key in vocab_keys if key.find(char) != -1]\n",
    "\n",
    "if prevent_square_brackets:\n",
    "  bad_keys.extend(find_keys(\"[\"))\n",
    "  bad_keys.extend(find_keys(\"]\"))\n",
    "\n",
    "if prevent_angle_brackets:\n",
    "  bad_keys.extend(find_keys(\"<\"))\n",
    "  bad_keys.extend(find_keys(\">\"))\n",
    "\n",
    "if prevent_curly_brackets:\n",
    "  bad_keys.extend(find_keys(\"{\"))\n",
    "  bad_keys.extend(find_keys(\"}\"))\n",
    "\n",
    "bad_words_ids = list()\n",
    "bad_keys_final = list()\n",
    "for key in bad_keys:\n",
    "  if key == \"<|endoftext|>\" or key in bad_keys_final:\n",
    "    continue\n",
    "  bad_id = vocab[key]\n",
    "  bad_words_ids.append([bad_id])\n",
    "  bad_keys_final.append(key)\n",
    "\n",
    "if len(bad_words_ids) < 1:\n",
    "  bad_words_ids = None\n",
    "\n",
    "#print(f\"Bad keys: {bad_keys_final} (Count: {len(bad_keys)})\")\n",
    "#print(f\"Bad ids: {bad_words_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFY FORWARD FUNCTION TO SPLIT MODEL\n",
    "number_of_parts = 32 #CAN BE 2 or 4 as well\n",
    "from transformers import GPTNeoModel\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "def new_forward(\n",
    "    self,\n",
    "    input_ids=None,\n",
    "    past_key_values=None,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    use_cache=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "):\n",
    "    global number_of_parts\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    if input_ids is not None and inputs_embeds is not None:\n",
    "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "    elif input_ids is not None:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        batch_size = input_ids.shape[0]\n",
    "    elif inputs_embeds is not None:\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "        batch_size = inputs_embeds.shape[0]\n",
    "    else:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
    "    if position_ids is not None:\n",
    "        position_ids = position_ids.view(-1, input_shape[-1])\n",
    "\n",
    "    if past_key_values is None:\n",
    "        past_length = 0\n",
    "        past_key_values = tuple([None] * len(self.h))\n",
    "    else:\n",
    "        past_length = past_key_values[0][0].size(-2)\n",
    "    if position_ids is None:\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "\n",
    "    # Attention mask.\n",
    "    if attention_mask is not None:\n",
    "        assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
    "        global_attention_mask = attention_mask.view(batch_size, -1)\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        global_attention_mask = global_attention_mask[:, None, None, :]\n",
    "\n",
    "        # Since global_attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        global_attention_mask = global_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        global_attention_mask = (1.0 - global_attention_mask) * -10000.0\n",
    "    else:\n",
    "        global_attention_mask = None\n",
    "\n",
    "    # Prepare head mask if needed\n",
    "    # 1.0 in head_mask indicate we keep the head\n",
    "    # attention_probs has shape bsz x num_headss x N x N\n",
    "    # head_mask has shape n_layer x batch x num_headss x N x N\n",
    "    head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "    position_embeds = self.wpe(position_ids)\n",
    "    hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "    if token_type_ids is not None:\n",
    "        token_type_embeds = self.wte(token_type_ids)\n",
    "        hidden_states = hidden_states + token_type_embeds\n",
    "\n",
    "    hidden_states = self.drop(hidden_states)\n",
    "\n",
    "    output_shape = input_shape + (hidden_states.size(-1),)\n",
    "\n",
    "    presents = () if use_cache else None\n",
    "    all_self_attentions = () if output_attentions else None\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "\n",
    "    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
    "        if number_of_parts == 2:\n",
    "            if i == 0:\n",
    "                cudastreams = {}\n",
    "                for j in range(0,16):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j+16].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                        \n",
    "                for j in range(0,16):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                        \n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "            if i == 16:\n",
    "                cudastreams = {}\n",
    "                for j in range(16,32):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j-16].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                        \n",
    "                for j in range(16,32):  \n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                            pass\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "        if number_of_parts == 4:\n",
    "            if i == 0:\n",
    "                cudastreams = {}\n",
    "                for j in range(0,8):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j+24].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                for j in range(0,8):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "            if i == 8:\n",
    "                cudastreams = {}\n",
    "                for j in range(8,16):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j-8].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                for j in range(8,16):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                    \n",
    "            if i == 16:\n",
    "                cudastreams = {}\n",
    "                for j in range(16,24):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j-8].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                for j in range(16,24):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        model.transformer.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "            if i == 24:\n",
    "                cudastreams = {}\n",
    "                for j in range(24,32):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j-8].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                for j in range(24,32):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "        if number_of_parts == 32:\n",
    "            \n",
    "            if i == 0:\n",
    "                for param1,param2 in zip(self.h[i].parameters(),self.h[31].parameters()):\n",
    "                    param1.data = param2.data\n",
    "                    \n",
    "                for param1,param2 in zip(self.h[0].parameters(),self.extrastorage[0].parameters()):\n",
    "                    param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "                self.h[0].to(\"cuda\", non_blocking=True)\n",
    "                    \n",
    "                    \n",
    "            if i >= 1:\n",
    "                for param1,param2 in zip(self.h[i].parameters(),self.h[i-1].parameters()):\n",
    "                    param1.data = param2.data\n",
    "                    \n",
    "                for param1,param2 in zip(self.h[i].parameters(),self.extrastorage[i].parameters()):\n",
    "                    param1.data.copy_(param2.data, non_blocking=True)\n",
    "                self.h[i].to(\"cuda\", non_blocking=True)\n",
    "        \n",
    "        attn_type = self.config.attention_layers[i]\n",
    "        attn_mask = global_attention_mask if attn_type == \"global\" else attention_mask\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "            if use_cache:\n",
    "                logger.warning(\n",
    "                    \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
    "                    \"`use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "            def create_custom_forward(module):\n",
    "                def custom_forward(*inputs):\n",
    "                    # None for past_key_value\n",
    "                    return module(*inputs, use_cache, output_attentions)\n",
    "\n",
    "                return custom_forward\n",
    "\n",
    "            outputs = torch.utils.checkpoint.checkpoint(\n",
    "                create_custom_forward(block),\n",
    "                hidden_states,\n",
    "                None,\n",
    "                attn_mask,\n",
    "                head_mask[i],\n",
    "            )\n",
    "        else:\n",
    "            outputs = block(\n",
    "                hidden_states,\n",
    "                layer_past=layer_past,\n",
    "                attention_mask=attn_mask,\n",
    "                head_mask=head_mask[i],\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if use_cache is True:\n",
    "            presents = presents + (outputs[1],)\n",
    "\n",
    "        if output_attentions:\n",
    "            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
    "\n",
    "    hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "    hidden_states = hidden_states.view(*output_shape)\n",
    "    # Add last hidden state\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    if not return_dict:\n",
    "        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "    return BaseModelOutputWithPast(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=presents,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_self_attentions,\n",
    "    )\n",
    "GPTNeoModel.forward = new_forward\n",
    "model.eval().half().to(\"cpu\")\n",
    "model.transformer.wte.to(\"cuda\")\n",
    "model.transformer.wpe.to(\"cuda\")\n",
    "model.transformer.ln_f.to(\"cuda\")\n",
    "model.lm_head.to(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "for param in model.transformer.wte.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.transformer.wpe.parameters():\n",
    "    param.requires_grad = False\n",
    "for i in range(32):\n",
    "    for param in model.transformer.h[i].parameters():\n",
    "        param.requires_grad = False\n",
    "for param in model.transformer.ln_f.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False\n",
    "setattr(model.transformer,\"extrastorage\",None)\n",
    "model.transformer.extrastorage = copy.deepcopy(model.transformer.h)\n",
    "smalltensor = torch.tensor(0).to(\"cuda\")\n",
    "for j in range(32):\n",
    "    for param1 in model.transformer.h[j].parameters():\n",
    "        param1.data = smalltensor\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.transformer.extrastorage.to(\"cpu\")\n",
    "for i in range(32):\n",
    "    for param in model.transformer.extrastorage[i].parameters():\n",
    "        param.requires_grad = False\n",
    "        param.data.pin_memory()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if number_of_parts == 2:\n",
    "    for j in range(16,32):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "            param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "        model.transformer.h[j].to(\"cuda\", non_blocking=True)  \n",
    "    print(\"number_of_parts = 4\" )\n",
    "    \n",
    "if number_of_parts == 4:\n",
    "    for j in range(24,32):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "            param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "        model.transformer.h[j].to(\"cuda\", non_blocking=True)  \n",
    "    print(\"number_of_parts = 4\" )\n",
    "    \n",
    "if number_of_parts == 32:\n",
    "    for param1,param2 in zip(model.transformer.h[31].parameters(),model.transformer.extrastorage[31].parameters()):\n",
    "        param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "    model.transformer.h[31].to(\"cuda\", non_blocking=True)  \n",
    "    print(\"number_of_parts = 32\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qd-HLb2nXaaA"
   },
   "outputs": [],
   "source": [
    "#@title Basic sampling\n",
    "\n",
    "#@markdown Use this cell if you just want to sample from the model in a free form way.\n",
    "\n",
    "basic_prompt = \"The rays of the evening sun falling in through the window bathed the room in a soft, warm light\" #@param {type:\"string\"}\n",
    "\n",
    "ids = tokenizer(basic_prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "n_ids = ids.shape[1]\n",
    "if n_ids < 1:\n",
    "  n_ids = 1\n",
    "  ids = torch.tensor([[tokenizer.eos_token_id]])\n",
    "max_length = n_ids + number_generated_tokens\n",
    "torch.cuda.empty_cache()\n",
    "basic_output = model.generate(\n",
    "    ids.long().cuda(),\n",
    "    do_sample=True,\n",
    "    min_length=max_length,\n",
    "    max_length=max_length,\n",
    "    temperature=temperature,\n",
    "    tfs = tail_free_sampling,\n",
    "    top_k = top_k,\n",
    "    top_p = top_p,\n",
    "    repetition_penalty = repetition_penalty,\n",
    "    repetition_penalty_range = repetition_penalty_range,\n",
    "    repetition_penalty_slope = repetition_penalty_slope,\n",
    "    use_cache=True,\n",
    "    bad_words_ids=bad_words_ids,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ").long().to(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GKfy1uQ9QdU"
   },
   "source": [
    "# Using the play function\n",
    "\n",
    "If your prompt starts with a letter, try putting a space or newline in front.\n",
    "\n",
    "* **generate** adds your prompt as an action and generates more output\n",
    "* **continue** generates more output\n",
    "* **edit** copies the last action into the prompt field and sets action to replace\n",
    "* **replace** replaces the last output with the prompt and generates more, use this to edit\n",
    "* **info** outputs LMI and memory\n",
    "* **history** outputs all actions so far\n",
    "* **memory** sets memory to the text in the prompt field\n",
    "* **authorsnote** sets author's note to the text in the prompt field\n",
    "* **andepth** sets the depth of the author's note to the number in the prompt\n",
    "* **tokenize** tokenizes the text in the prompt field and outputs the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fb_NHHISOeYg"
   },
   "outputs": [],
   "source": [
    "#@title Play\n",
    "\n",
    "action_type = \"generate\"\n",
    "prompt = \"\"\n",
    "need_refresh = True\n",
    "\n",
    "action_types = [\"generate\", \"continue\", \"edit\", \"replace\", \"undo\", \"retry\", \"memory\", \"authorsnote\", \"andepth\", \"info\", \"history\", \"tokenize\"]\n",
    "\n",
    "def assemble():\n",
    "  remaining = (2048 - number_generated_tokens + 1) - memory[1].shape[1] - an[1].shape[1]\n",
    "  n_actions = len(actions)\n",
    "  n_ctx = 0\n",
    "  back_i = n_actions\n",
    "  for i in range(n_actions):\n",
    "      i_action = n_actions - i - 1\n",
    "      n_tok = actions[i_action][1].shape[1]\n",
    "      if remaining > n_ctx + n_tok:\n",
    "        n_ctx += n_tok\n",
    "        back_i = i_action\n",
    "      else:\n",
    "        break\n",
    "  lmi[0], lmi[1] = memory[0], memory[1]\n",
    "  start = False\n",
    "  if n_actions - back_i - 1 < an_depth:\n",
    "    start = True\n",
    "  while back_i < n_actions:\n",
    "    if start or n_actions - back_i - 1 == an_depth:\n",
    "      lmi[0] += an[0]\n",
    "      lmi[1] = torch.cat([lmi[1].cpu(), an[1].cpu()], 1).long()\n",
    "      start = False\n",
    "    lmi[0] += actions[back_i][0]\n",
    "    lmi[1] = torch.cat([lmi[1].cpu(), actions[back_i][1].cpu()], 1).long()\n",
    "    back_i += 1\n",
    "\n",
    "def clear_output():\n",
    "  with out:\n",
    "    IPython.display.clear_output()\n",
    "\n",
    "def set_action(change):\n",
    "  global action_type\n",
    "  action_type = change.new\n",
    "\n",
    "def set_prompt(change):\n",
    "  global prompt\n",
    "  prompt = change.new\n",
    "\n",
    "@torch.no_grad()\n",
    "def play(do_action=None):\n",
    "  global memory, need_refresh, an, an_depth, action_type, history\n",
    "  an_updated = False\n",
    "  memory_updated = False\n",
    "  if do_action is not None:\n",
    "    action = do_action\n",
    "    action_type = do_action\n",
    "  else:\n",
    "    action = action_type\n",
    "  with out:\n",
    "    if prompt in action_types:\n",
    "      action == prompt\n",
    "    else:\n",
    "      if action == \"edit\":\n",
    "        if len(actions) > 0:\n",
    "          input.value = actions[-1][0]\n",
    "        else:\n",
    "          input.value = \"\"\n",
    "        dropdown.value = \"replace\"\n",
    "        return\n",
    "      if action == \"replace\":\n",
    "        if len(actions) > 0:\n",
    "          actions.pop()\n",
    "        need_refresh = True\n",
    "        action = \"generate\"\n",
    "      if action == \"generate\":\n",
    "        text = prompt\n",
    "        if len(text) > 0:\n",
    "          for line in text.splitlines(True):\n",
    "            tokens = tokenizer(line, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "            actions.append((line, tokens))\n",
    "        action = \"continue\"\n",
    "      if action == \"info\":\n",
    "        clear_output()\n",
    "        print(\"LMI: \" + lmi[0])\n",
    "        print(\"LMI tokens: \" + str(lmi[1].shape[1]))\n",
    "        print(\"Memory: \" + memory[0])\n",
    "        print(\"Author's note: \" + an[0])\n",
    "        print(\"Author's note depth: \" + str(an_depth))\n",
    "        need_refresh = True\n",
    "      if action == \"history\":\n",
    "        clear_output()\n",
    "        print(\"\".join([action[0] for action in actions]), end=\"\")\n",
    "        need_refresh = False\n",
    "      if action == \"retry\":\n",
    "        if len(actions) > 0:\n",
    "          actions.pop()\n",
    "        need_refresh = True\n",
    "        action = \"continue\"\n",
    "      if action == \"undo\":\n",
    "        if len(actions) > 0:\n",
    "          actions.pop()\n",
    "        assemble()\n",
    "        clear_output()\n",
    "        print(\"\".join([action[0] for action in actions[-number_show_last_actions:]]), end=\"\")\n",
    "        need_refresh = False\n",
    "      if action == \"memory\":\n",
    "        if prompt == \"\":\n",
    "          memory = (\"\", torch.zeros((1, 0)).long())\n",
    "          text = \"\"\n",
    "        else:\n",
    "          text = codecs.decode(prompt + \"\\n\", \"unicode-escape\")\n",
    "          tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "          memory = (text, tokens)\n",
    "        clear_output()\n",
    "        print(\"Memory: \" + text)\n",
    "        memory_updated = True\n",
    "      if action == \"authorsnote\":\n",
    "        if prompt == \"\":\n",
    "          an = (\"\", torch.zeros((1, 0)).long())\n",
    "          text = \"\"\n",
    "        else:\n",
    "          text = \"\\n[Author's note: \" + codecs.decode(prompt, \"unicode-escape\") + \"]\\n\"\n",
    "          tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "          an = (text, tokens)\n",
    "        clear_output()\n",
    "        print(\"Author's note: \" + text)\n",
    "        an_updated = True\n",
    "      if action == \"andepth\":\n",
    "        clear_output()\n",
    "        try:\n",
    "          an_depth = int(codecs.decode(prompt + \"\\n\", \"unicode-escape\"))\n",
    "        except:\n",
    "          pass\n",
    "        print(\"Author's note depth: \" + str(an_depth))\n",
    "        an_updated = True\n",
    "      if action == \"tokenize\":\n",
    "        text = codecs.decode(prompt, \"unicode-escape\")\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "        clear_output()\n",
    "        print(\"Tokens: \" + str(tokens.shape[1]))\n",
    "        print(tokens[0])\n",
    "        need_refresh = True\n",
    "      if action == \"continue\":\n",
    "        assemble()\n",
    "        ids = lmi[1].cuda()\n",
    "        n_ids = ids.shape[1]\n",
    "        if n_ids < 1:\n",
    "          n_ids = 1\n",
    "          ids = torch.tensor([[tokenizer.eos_token_id]])\n",
    "        max_length = number_generated_tokens + n_ids\n",
    "        #ids[:, :] = 13\n",
    "        torch.cuda.empty_cache()\n",
    "        clear_output()\n",
    "        gen_tokens = model.generate(\n",
    "            ids.long().cuda(),\n",
    "            do_sample=True,\n",
    "            min_length=max_length,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            tfs = tail_free_sampling,\n",
    "            top_k = top_k,\n",
    "            top_p = top_p,\n",
    "            repetition_penalty = repetition_penalty,\n",
    "            repetition_penalty_range = repetition_penalty_range,\n",
    "            repetition_penalty_slope = repetition_penalty_slope,\n",
    "            use_cache=True,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        ).long()\n",
    "        stop_tokens = [0, 13, 30, 526, 764, 1701, 2474, 5145, 5633]\n",
    "        for i in reversed(range(len(gen_tokens[0]))):\n",
    "          if i < n_ids:\n",
    "            gen_tokens = gen_tokens[0]\n",
    "            break\n",
    "          if gen_tokens[0][i] in stop_tokens:\n",
    "            gen_tokens = gen_tokens[0][:i+1]\n",
    "            break\n",
    "        gen_text = tokenizer.decode(gen_tokens[n_ids:])\n",
    "        if len(gen_text) > 0:\n",
    "          actions.append((gen_text, gen_tokens[n_ids:].unsqueeze(0).cpu()))\n",
    "        print(\"\".join([action[0] for action in actions[-number_show_last_actions:]]), end=\"\")\n",
    "        torch.cuda.empty_cache()\n",
    "        need_refresh = False\n",
    "    if history is not None:\n",
    "      if history:\n",
    "        with out_history:\n",
    "          IPython.display.clear_output()\n",
    "          print(\"\".join([action[0] for action in actions]), end=\"\")\n",
    "        with out_history2:\n",
    "          IPython.display.clear_output()\n",
    "      else:\n",
    "        with out_history2:\n",
    "          IPython.display.clear_output()\n",
    "          print(\"\".join([action[0] for action in actions]), end=\"\")\n",
    "        with out_history:\n",
    "          IPython.display.clear_output()\n",
    "      if an_updated:\n",
    "        with out_an:\n",
    "          IPython.display.clear_output()\n",
    "          if len(an[0]) > 0:\n",
    "            print(\"AN depth: \" + str(an_depth) + \"\\n\" + an[0], end=\"\")\n",
    "      if memory_updated:\n",
    "        with out_memory:\n",
    "          IPython.display.clear_output()\n",
    "          print(memory[0], end=\"\")\n",
    "      history = not history\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import IPython.display\n",
    "out = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
    "dropdown = widgets.Dropdown(options=action_types, value=action_type, description='Action:', disabled=False)\n",
    "dropdown.observe(set_action, 'value')\n",
    "button = widgets.Button(description='[selected action]', disabled=False)\n",
    "button.on_click(lambda _: play(dropdown.value))\n",
    "generate_button = widgets.Button(description='Generate', disabled=False)\n",
    "generate_button.on_click(lambda _: play(\"generate\"))\n",
    "edit_button = widgets.Button(description='Edit', disabled=False)\n",
    "edit_button.on_click(lambda _: play(\"edit\"))\n",
    "retry_button = widgets.Button(description='Retry', disabled=False)\n",
    "retry_button.on_click(lambda _: play(\"retry\"))\n",
    "continue_button = widgets.Button(description='Continue', disabled=False)\n",
    "continue_button.on_click(lambda _: play(\"continue\"))\n",
    "undo_button = widgets.Button(description='Undo', disabled=False)\n",
    "undo_button.on_click(lambda _: play(\"undo\"))\n",
    "dropdown_hbox = widgets.HBox([dropdown, button])\n",
    "hbox = widgets.HBox([generate_button, edit_button, retry_button, continue_button, undo_button])\n",
    "input = widgets.Textarea(value='', placeholder='', description='Input:', disabled=False, rows=4, layout={\"width\": \"1280px\"})\n",
    "input.observe(set_prompt, 'value')\n",
    "\n",
    "display(out, dropdown_hbox, hbox, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jPtYoUNO39Mg"
   },
   "outputs": [],
   "source": [
    "#@title History\n",
    "#@markdown Run this cell to have an auto-updating full listing of the current story.\n",
    "\n",
    "history = True\n",
    "out_history = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
    "out_history2 = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
    "out_memory = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
    "out_an = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
    "display(out_history, out_history2, out_memory, out_an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qvIvzStC9rY9"
   },
   "outputs": [],
   "source": [
    "#@title Attention display\n",
    "#@markdown If you don't know what this is, just ignore. attn_head_combination selects the operation used to combine layer and attention head results.\n",
    "\n",
    "visualizer_prompt = \"Before his eyes was an orange cat with stripes. Running his fingers through its soft fur, he admired\" #@param {type:\"string\"}\n",
    "max_attentions =  8#@param {type:\"integer\"}\n",
    "attn_head_combination = \"mean\" #@param [\"mean\", \"max\"]\n",
    "use_lmi_as_prompt = False #@param {type:\"boolean\"}\n",
    "\n",
    "if use_lmi_as_prompt:\n",
    "  ids = lmi[1]\n",
    "else:\n",
    "  ids = tokenizer(visualizer_prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "\n",
    "n_ids = ids.shape[1]\n",
    "if n_ids < 1:\n",
    "  n_ids = 1\n",
    "  ids = torch.tensor([[tokenizer.eos_token_id]])\n",
    "\n",
    "max_length = n_ids + number_generated_tokens\n",
    "torch.cuda.empty_cache()\n",
    "basic_output = model.generate(\n",
    "    ids.long().cuda(),\n",
    "    do_sample=True,\n",
    "    min_length=max_length,\n",
    "    max_length=max_length,\n",
    "    temperature=temperature,\n",
    "    tfs = tail_free_sampling,\n",
    "    top_k = top_k,\n",
    "    top_p = top_p,\n",
    "    repetition_penalty = repetition_penalty,\n",
    "    repetition_penalty_range = repetition_penalty_range,\n",
    "    repetition_penalty_slope = repetition_penalty_slope,\n",
    "    use_cache=True,\n",
    "    bad_words_ids=bad_words_ids,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    return_dict_in_generate=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "attentions = basic_output[\"attentions\"]\n",
    "basic_output = basic_output[\"sequences\"].cpu()\n",
    "\n",
    "print(\"Prompt: \" + visualizer_prompt)\n",
    "print()\n",
    "\n",
    "def combine(attentions):\n",
    "  if attn_head_combination == \"mean\":\n",
    "    attentions = attentions.mean(0)\n",
    "  else:\n",
    "    attentions = attentions.max(0)[0]\n",
    "  return attentions\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "#gen_tokens x layers x tensor (1 x heads x n_ids square)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "for i in range(number_generated_tokens):\n",
    "  layer_attn = []\n",
    "  for j in range(0,len(attentions[i])):\n",
    "    layer_attn.append(combine(attentions[i][j][0,:,-1].float().cpu()))\n",
    "  layer_attn = torch.stack(layer_attn)\n",
    "  token_attn = combine(layer_attn)\n",
    "  prob, topk = token_attn.topk(max_attentions)\n",
    "  top_tokens = []\n",
    "  for top in topk:\n",
    "    decoded = tokenizer.decode(torch.tensor([basic_output[0][top]]))\n",
    "    top_tokens.append(f\"{decoded!r}@{top} ({token_attn[top]:.4f})\")\n",
    "  print(\"Token: \" + repr(tokenizer.decode(torch.tensor([basic_output[0][n_ids + i]]))))\n",
    "  print(\", \".join(top_tokens))\n",
    "  print()\n",
    "\n",
    "print(\"Output: \" + tokenizer.decode(basic_output[0]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dungeon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
